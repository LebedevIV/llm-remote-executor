# LLM Remote Executor

Простой, но мощный API-сервер на Node.js, который позволяет языковым моделям (LLM) безопасно взаимодействовать с файловой системой и выполнять команды на вашем компьютере или удаленном сервере.

## Философия

Этот экспериментальный проект — не замена IDE-инструментам вроде Copilot или Gemini CLI, а предложение **альтернативной парадигмы** работы, где ИИ выступает не как "помощник", а как центральный "исполнитель" (агент), а вы — как оператор, предоставляющий ему доступ к инструментам. Это дает 100% контроль, прозрачность и свободу выбора любой языковой модели. Для максимальной простоты и контроля используются исключительно **GET-запросы**.

---

## Часть 1: Локальная установка (Рекомендуемый способ)

Это самый быстрый и простой способ начать работу. Сервер будет работать на вашем компьютере и иметь доступ только к указанной вами локальной папке.

### Шаг 1: Подготовка

1.  Установите [Node.js](https://nodejs.org/) (версию LTS).
2.  Создайте папку для проекта и перейдите в нее:
    ```bash
    mkdir llm-remote-executor
    cd llm-remote-executor
    ```
3.  Инициализируйте проект и установите `express`:
    ```bash
    npm init -y
    npm install express
    ```

### Шаг 2: Создание файлов

Создайте в папке проекта два ключевых файла: `server.js` и `config.json`.

**`config.json` (Файл конфигурации):**
Это ваш центр управления.
*   `PORT`: Порт, на котором будет работать сервер.
*   `SECRET_TOKEN`: Ваш секретный ключ для доступа к API.
*   `BASE_DIR`: **Самая важная настройка.** Путь к папке, в которой ИИ сможет работать. Может быть относительным (`./workspace`) или абсолютным (`/home/user/projects`).

Пример `config.json`:
```json
{
  "PORT": 3000,
  "SECRET_TOKEN": "your-super-secret-token-here",
  "BASE_DIR": "./local-workspace"
}

**server.js:**
Скопируйте код для сервера из этого репозитория. Этот файл содержит всю логику API.

### Шаг 3: Запуск

1.  Откройте терминал в папке проекта.
2.  Запустите сервер командой:
    `node server.js`
3.  Вы увидите сообщение о том, что сервер запущен и готов к работе.

---

## Часть 2: Развертывание на удаленном сервере (Опционально)

Этот раздел для тех, кто хочет, чтобы их API был доступен из любой точки мира через интернет. Для этого потребуются стандартные инструменты развертывания, такие как `pm2` (для фоновой работы) и `Nginx` (в качестве реверс-прокси).

---

## Часть 3: Системная Инструкция для ИИ и Примеры

### 1. Спецификация API

- **Базовый URL (BASE_URL):** `http://localhost:3000` (или ваш удаленный домен)
- **Эндпоинт:** `/api`
- **Обязательный токен авторизации:** `token={{ВАШ_СЕКРЕТНЫЙ_ТОКЕН}}`

**Действия (параметр `action`):**
- `shell`: Выполняет команду. **Параметр:** `command`.
- `write_file`: Записывает файл. **Параметры:** `path`, `content`.
- `read_file`: Читает файл. **Параметр:** `path`.
- `list_dir`: Список директории. **Параметр:** `path` (опционально).

*Все параметры (`command`, `path`, `content`) должны быть URL-кодированы.*

**Примеры `curl`:**

Запрос списка файлов (простой GET-запрос):
```bash
curl "http://localhost:3000/api?token=...&action=list_dir"
```

Запись в файл (требует флага `-G`):
При использовании `curl` с флагом `--data-urlencode` для передачи содержимого файла, `curl` по умолчанию переключается на метод POST. Поскольку наш сервер принимает только GET, необходимо **обязательно использовать флаг `-G`**, чтобы сохранить метод GET.
```bash
curl -G "http://localhost:3000/api?token=...&action=write_file&path=test.txt" --data-urlencode "content=Hello World"
```

### 2. Рабочий процесс: "Выполнение при посредничестве `curl`"

Твое взаимодействие с сервером происходит по строгому циклу "План-Генерация-Ожидание".

1.  **Пользователь дает тебе задачу.**
2.  **Ты формируешь ПЛАН ВЫПОЛНЕНИЯ.**
3.  **Ты генерируешь ОДНУ `curl` команду** для выполнения первого шага.
4.  **Ты ожидаешь,** пока пользователь выполнит эту команду и предоставит тебе **ПОЛНЫЙ ВЫВОД** из терминала.
5.  **Ты анализируешь результат** и переходишь к следующему шагу.
